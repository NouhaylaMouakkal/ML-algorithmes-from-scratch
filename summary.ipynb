{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute distances between x and all examples in the training set\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        # Sort by distance and return indices of the first k neighbors\n",
    "        k_idx = np.argsort(distances)[: self.k]\n",
    "        # Extract the labels of the k nearest neighbor training samples\n",
    "        k_neighbor_labels = [self.y_train[i] for i in k_idx]\n",
    "        # return the most common class label\n",
    "        most_common = Counter(k_neighbor_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    from sklearn import datasets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    cmap = ListedColormap([\"#FF0000\", \"#00FF00\", \"#0000FF\"])\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=1234\n",
    "    )\n",
    "\n",
    "    k = 3\n",
    "    clf = KNN(k=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(\"KNN classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "clf = KNN(k=3)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "acc = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"KNN classification accuracy: {acc}\")\n",
    "print(\"KNN classification accuracy\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    corr_matrix = np.corrcoef(y_true, y_pred)\n",
    "    corr = corr_matrix[0, 1]\n",
    "    return corr ** 2\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_approximated = np.dot(X, self.weights) + self.bias\n",
    "        return y_approximated\n",
    "\n",
    "\n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import datasets\n",
    "\n",
    "    def mean_squared_error(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    X, y = datasets.make_regression(\n",
    "        n_samples=100, n_features=1, noise=20, random_state=4\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=1234\n",
    "    )\n",
    "\n",
    "    regressor = LinearRegression(learning_rate=0.01, n_iters=1000)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    predictions = regressor.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(\"MSE:\", mse)\n",
    "\n",
    "    accu = r2_score(y_test, predictions)\n",
    "    print(\"Accuracy:\", accu)\n",
    "\n",
    "    y_pred_line = regressor.predict(X)\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    m1 = plt.scatter(X_train, y_train, color=cmap(0.9), s=10)\n",
    "    m2 = plt.scatter(X_test, y_test, color=cmap(0.5), s=10)\n",
    "    plt.plot(X, y_pred_line, color=\"black\", linewidth=2, label=\"Prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            # approximate y with linear combination of weights and x, plus bias\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            # apply sigmoid function\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            # update parameters\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import datasets\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    bc = datasets.load_breast_cancer()\n",
    "    X, y = bc.data, bc.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=1234\n",
    "    )\n",
    "\n",
    "    regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    predictions = regressor.predict(X_test)\n",
    "\n",
    "    print(\"LR classification accuracy:\", accuracy(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "acc = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"LR classification accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Mean centering\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X = X - self.mean\n",
    "\n",
    "        # covariance, function needs samples as columns\n",
    "        cov = np.cov(X.T)\n",
    "\n",
    "        # eigenvalues, eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "        # -> eigenvector v = [:,i] column vector, transpose for easier calculations\n",
    "        # sort eigenvectors\n",
    "        eigenvectors = eigenvectors.T\n",
    "        idxs = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idxs]\n",
    "        eigenvectors = eigenvectors[idxs]\n",
    "\n",
    "        # store first n eigenvectors\n",
    "        self.components = eigenvectors[0 : self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        # project data\n",
    "        X = X - self.mean\n",
    "        return np.dot(X, self.components.T)\n",
    "\n",
    "\n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Imports\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn import datasets\n",
    "\n",
    "    # data = datasets.load_digits()\n",
    "    data = datasets.load_iris()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    # Project the data onto the 2 primary principal components\n",
    "    pca = PCA(2)\n",
    "    pca.fit(X)\n",
    "    X_projected = pca.transform(X)\n",
    "\n",
    "    print(\"Shape of X:\", X.shape)\n",
    "    print(\"Shape of transformed X:\", X_projected.shape)\n",
    "\n",
    "    x1 = X_projected[:, 0]\n",
    "    x2 = X_projected[:, 1]\n",
    "\n",
    "    plt.scatter(\n",
    "        x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 3)\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumer de cours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Lin Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.00002, n_iterations=10000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = None\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        self.dataset = pd.read_csv(filepath)\n",
    "        return self.dataset\n",
    "\n",
    "    def visualize_data(self):\n",
    "        numeric_columns = self.dataset.drop(columns=['Sales'])\n",
    "        plt.figure()\n",
    "        sns.heatmap(numeric_columns.corr(), annot=True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        for i, feature in enumerate(['TV', 'Radio', 'Newspaper'], 1):\n",
    "            plt.subplot(1, 3, i)\n",
    "            plt.scatter(self.dataset[feature], self.dataset['Sales'])\n",
    "            plt.title(f'{feature} vs Sales')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel('Sales')\n",
    "        plt.show()\n",
    "\n",
    "    def prepare_data(self, feature):\n",
    "        self.y = self.dataset['Sales'].values.reshape(-1, 1)\n",
    "        self.X = self.dataset[feature].values.reshape(-1, 1)\n",
    "\n",
    "        # Normalisation des features\n",
    "        scaler = StandardScaler()\n",
    "        self.X = scaler.fit_transform(self.X)\n",
    "\n",
    "        # Ajout d'une colonne de biais\n",
    "        self.X = np.hstack((self.X, np.ones((self.X.shape[0], 1))))\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "        self.theta = np.random.randn(2, 1)\n",
    "\n",
    "    def model(self, X):\n",
    "        return X.dot(self.theta)\n",
    "\n",
    "    def cost_function(self, X, y):\n",
    "        m = len(y)\n",
    "        return 1 / (2 * m) * np.sum((self.model(X) - y) ** 2)\n",
    "\n",
    "    def grad(self, X, y):\n",
    "        m = len(y)\n",
    "        return 1 / m * X.T.dot(self.model(X) - y)\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        self.cost_history = np.zeros(self.n_iterations)\n",
    "        for i in range(self.n_iterations):\n",
    "            self.theta -= self.learning_rate * self.grad(self.X_train, self.y_train)\n",
    "            self.cost_history[i] = self.cost_function(self.X_train, self.y_train)\n",
    "        return self.theta, self.cost_history\n",
    "\n",
    "    def train(self):\n",
    "        return self.gradient_descent()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "    def plot_results(self):\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.X_train[:, 0], self.y_train, 'o', label='Training set')\n",
    "        plt.plot(self.X_train[:, 0], self.model(self.X_train), 'r-', label='Linear model')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Sales')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(self.n_iterations), self.cost_history)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Cost Function History')\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self):\n",
    "        def coef_determination(y, pred):\n",
    "            u = ((y - pred) ** 2).sum()\n",
    "            v = ((y - y.mean()) ** 2).sum()\n",
    "            return 1 - u / v\n",
    "\n",
    "        y_train_pred = self.predict(self.X_train)\n",
    "        y_test_pred = self.predict(self.X_test)\n",
    "\n",
    "        print(f\"Training R^2: {coef_determination(self.y_train, y_train_pred)}\")\n",
    "        print(f\"Test R^2: {coef_determination(self.y_test, y_test_pred)}\")\n",
    "\n",
    "# Utilisation de la classe pour TV\n",
    "regression = LinearRegression()\n",
    "dataset = regression.load_data(\"Advertising.csv\")\n",
    "regression.visualize_data()\n",
    "regression.prepare_data('TV')\n",
    "theta, cost_history = regression.train()\n",
    "regression.plot_results()\n",
    "regression.evaluate()\n",
    "\n",
    "# Utilisation de la classe pour Newspaper\n",
    "regression.prepare_data('Newspaper')\n",
    "theta, cost_history = regression.train()\n",
    "regression.plot_results()\n",
    "regression.evaluate()\n",
    "\n",
    "# Utilisation de la classe pour Radio\n",
    "regression.prepare_data('Radio')\n",
    "theta, cost_history = regression.train()\n",
    "regression.plot_results()\n",
    "regression.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg Lin Multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class LinearRegressionMultiple:\n",
    "    def __init__(self, learning_rate=0.00004, n_iterations=1000000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.theta = None\n",
    "        self.cost_history = None\n",
    "\n",
    "    def load_data(self, filepath):\n",
    "        self.dataset = pd.read_csv(filepath)\n",
    "        return self.dataset\n",
    "\n",
    "    def visualize_data(self):\n",
    "        numeric_columns = self.dataset.drop(columns=['Sales'])\n",
    "        plt.figure()\n",
    "        sns.heatmap(numeric_columns.corr(), annot=True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        features = ['TV', 'Radio', 'Newspaper']\n",
    "        for i, feature in enumerate(features, 1):\n",
    "            plt.subplot(1, 3, i)\n",
    "            plt.scatter(self.dataset[feature], self.dataset['Sales'])\n",
    "            plt.title(f'{feature} vs Sales')\n",
    "            plt.xlabel(feature)\n",
    "            plt.ylabel('Sales')\n",
    "        plt.show()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.y = self.dataset['Sales'].values.reshape(-1, 1)\n",
    "        self.X = self.dataset[['TV', 'Radio', 'Newspaper']].values\n",
    "\n",
    "        # Normalisation des features\n",
    "        scaler = StandardScaler()\n",
    "        self.X = scaler.fit_transform(self.X)\n",
    "\n",
    "        # Ajout d'une colonne de biais\n",
    "        self.X = np.hstack((self.X, np.ones((self.X.shape[0], 1))))\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "        self.theta = np.random.randn(self.X_train.shape[1], 1)\n",
    "\n",
    "    def model(self, X):\n",
    "        return X.dot(self.theta)\n",
    "\n",
    "    def cost_function(self, X, y):\n",
    "        m = len(y)\n",
    "        return 1 / (2 * m) * np.sum((self.model(X) - y) ** 2)\n",
    "\n",
    "    def grad(self, X, y):\n",
    "        m = len(y)\n",
    "        return 1 / m * X.T.dot(self.model(X) - y)\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        self.cost_history = np.zeros(self.n_iterations)\n",
    "        for i in range(self.n_iterations):\n",
    "            self.theta -= self.learning_rate * self.grad(self.X_train, self.y_train)\n",
    "            self.cost_history[i] = self.cost_function(self.X_train, self.y_train)\n",
    "        return self.theta, self.cost_history\n",
    "\n",
    "    def train(self):\n",
    "        return self.gradient_descent()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "    def plot_results(self):\n",
    "        fig = plt.figure(figsize=(18, 5))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        sc = ax.scatter(self.X_train[:, 0], self.X_train[:, 1], self.X_train[:, 2], c=self.y_train, cmap='viridis')\n",
    "        cbar = plt.colorbar(sc, label='Sales')\n",
    "        ax.set_xlabel('TV')\n",
    "        ax.set_ylabel('Radio')\n",
    "        ax.set_zlabel('Newspaper')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(range(self.n_iterations), self.cost_history)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Cost Function History')\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self):\n",
    "        def coef_determination(y, pred):\n",
    "            u = ((y - pred) ** 2).sum()\n",
    "            v = ((y - y.mean()) ** 2).sum()\n",
    "            return 1 - u / v\n",
    "\n",
    "        y_train_pred = self.predict(self.X_train)\n",
    "        y_test_pred = self.predict(self.X_test)\n",
    "\n",
    "        print(f\"Training R^2: {coef_determination(self.y_train, y_train_pred)}\")\n",
    "        print(f\"Test R^2: {coef_determination(self.y_test, y_test_pred)}\")\n",
    "\n",
    "# Utilisation de la classe\n",
    "regression_multiple = LinearRegressionMultiple()\n",
    "dataset = regression_multiple.load_data(\"Advertising.csv\")\n",
    "regression_multiple.visualize_data()\n",
    "regression_multiple.prepare_data()\n",
    "theta_final, cost_history = regression_multiple.train()\n",
    "regression_multiple.plot_results()\n",
    "regression_multiple.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regr Polynomial Univarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "%matplotlib inline\n",
    "\n",
    "# Chargement du dataset Iris\n",
    "dataset = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "# Visualisation des données\n",
    "plt.plot(dataset[\"petal_length\"], dataset[\"sepal_width\"], 'o')\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('sepal_width')\n",
    "plt.show()\n",
    "\n",
    "# Préparation des données\n",
    "x = dataset.petal_length.values.reshape(-1, 1)\n",
    "y = dataset.sepal_width.values.reshape(-1, 1)\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Ajout de termes polynomiaux et d'une colonne de biais\n",
    "X_poly = np.hstack((x_scaled**2, x_scaled, np.ones(x_scaled.shape)))\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialisation des paramètres du modèle\n",
    "np.random.seed(0)\n",
    "theta = np.random.randn(X_train.shape[1], 1)\n",
    "\n",
    "# Définition des fonctions du modèle, de coût, de gradient et de descente de gradient\n",
    "def model(X, theta):\n",
    "    return X.dot(theta)\n",
    "\n",
    "def cost_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    return 1 / (2 * m) * np.sum((model(X, theta) - y) ** 2)\n",
    "\n",
    "def grad(X, y, theta):\n",
    "    m = len(y)\n",
    "    return 1 / m * X.T.dot(model(X, theta) - y)\n",
    "\n",
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    cost_history = np.zeros(n_iterations)\n",
    "    for i in range(n_iterations):\n",
    "        theta -= learning_rate * grad(X, y, theta)\n",
    "        cost_history[i] = cost_function(X, y, theta)\n",
    "    return theta, cost_history\n",
    "\n",
    "# Phase d'entraînement\n",
    "n_iterations = 40000\n",
    "learning_rate = 0.004\n",
    "theta_final, cost_history = gradient_descent(X_train, y_train, theta, learning_rate, n_iterations)\n",
    "\n",
    "# Visualisation des résultats\n",
    "plt.plot(x, y, 'o', label='dataset')\n",
    "plt.plot(x, model(X_poly, theta_final), c='r', label='modèle final')\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('sepal_width')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "predictions = model(X_test, theta_final)\n",
    "plt.plot(x_test, y_test, 'o', label='dataset test')\n",
    "plt.plot(x_test, predictions, c='r', label='prédictions')\n",
    "plt.xlabel('petal_length')\n",
    "plt.ylabel('sepal_width')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Courbe d'apprentissage\n",
    "plt.plot(range(n_iterations), cost_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function History')\n",
    "plt.show()\n",
    "\n",
    "# Évaluation du modèle - Coefficient de détermination\n",
    "def coef_determination(y, pred):\n",
    "    u = ((y - pred) ** 2).sum()\n",
    "    v = ((y - y.mean()) ** 2).sum()\n",
    "    return 1 - u / v\n",
    "\n",
    "# Calcul du coefficient de détermination pour l'ensemble de test\n",
    "print(f\"Test R^2: {coef_determination(y_test, predictions)}\")\n",
    "\n",
    "# Information sur le dataset\n",
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reg POly Multivariee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class PolynomialRegressionModel:\n",
    "    def __init__(self, data_path, target_col, feature_col, max_degree=17, test_size=0.3, random_state=101):\n",
    "        self.data_path = data_path\n",
    "        self.target_col = target_col\n",
    "        self.feature_col = feature_col\n",
    "        self.max_degree = max_degree\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        self.x = self.df[[self.feature_col]].values\n",
    "        self.y = self.df[[self.target_col]].values.reshape(-1, 1)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size=self.test_size, random_state=self.random_state)\n",
    "        self.X_train = np.array(self.X_train).reshape(-1, 1)\n",
    "        self.X_test = np.array(self.X_test).reshape(-1, 1)\n",
    "\n",
    "    def plot_corr_matrix(self):\n",
    "        plt.figure()\n",
    "        sns.heatmap(self.df.corr(), annot=True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_data(self):\n",
    "        plt.plot(self.y, self.x, 'o')\n",
    "        plt.xlabel(self.feature_col)\n",
    "        plt.ylabel(self.target_col)\n",
    "        plt.show()\n",
    "\n",
    "    def select_best_polynomial_degree(self):\n",
    "        bic_values = []\n",
    "        models = []\n",
    "\n",
    "        for degree in range(1, self.max_degree + 1):\n",
    "            poly_features = PolynomialFeatures(degree=degree)\n",
    "            X_train_poly = poly_features.fit_transform(self.X_train)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_poly, self.y_train)\n",
    "            num_params = X_train_poly.shape[1]\n",
    "            y_train_pred = model.predict(X_train_poly)\n",
    "            log_likelihood = -0.5 * len(self.X_train) * np.log(mean_squared_error(self.y_train, y_train_pred))\n",
    "            bic = -2 * log_likelihood + num_params * np.log(len(self.X_train))\n",
    "            bic_values.append(bic)\n",
    "            models.append(model)\n",
    "\n",
    "        self.best_model_idx = np.argmin(bic_values)\n",
    "        self.best_model = models[self.best_model_idx]\n",
    "        self.best_degree = self.best_model_idx + 1\n",
    "\n",
    "        plt.plot(range(1, self.max_degree + 1), bic_values, marker='o')\n",
    "        plt.xlabel('Polynomial Degree')\n",
    "        plt.ylabel('BIC Value')\n",
    "        plt.title('BIC Values vs. Polynomial Degree')\n",
    "        plt.xticks(range(1, self.max_degree + 1))\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def prepare_polynomial_features(self, degree):\n",
    "        poly_features = PolynomialFeatures(degree=degree)\n",
    "        self.X_train_poly = poly_features.fit_transform(self.X_train)\n",
    "        self.X_test_poly = poly_features.fit_transform(self.X_test)\n",
    "        self.X_train_poly_norm, self.mean, self.std = self.z_score_normalization(self.X_train_poly)\n",
    "        self.X_test_poly_norm = (self.X_test_poly - self.mean) / self.std\n",
    "        self.X_train_poly_norm = np.hstack((self.X_train_poly_norm, np.ones((self.X_train_poly.shape[0], 1))))\n",
    "        self.X_test_poly_norm = np.hstack((self.X_test_poly_norm, np.ones((self.X_test_poly.shape[0], 1))))\n",
    "        self.theta = np.random.randn(self.X_train_poly_norm.shape[1], 1)\n",
    "\n",
    "    def z_score_normalization(self, data):\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0)\n",
    "        data_normalized = (data - mean) / std\n",
    "        return data_normalized, mean, std\n",
    "\n",
    "    def train_model(self, learning_rate=0.05, n_iterations=10000):\n",
    "        self.theta_final, self.cost_history = self.gradient_descent(self.X_train_poly_norm, self.y_train, self.theta, learning_rate, n_iterations)\n",
    "\n",
    "    def model(self, X, theta):\n",
    "        return X.dot(theta)\n",
    "\n",
    "    def cost_function(self, X, y, theta):\n",
    "        m = len(y)\n",
    "        return 1 / (2 * m) * np.sum((self.model(X, theta) - y) ** 2)\n",
    "\n",
    "    def grad(self, X, y, theta):\n",
    "        m = len(y)\n",
    "        return 1 / m * X.T.dot(self.model(X, theta) - y)\n",
    "\n",
    "    def gradient_descent(self, X, y, theta, learning_rate, n_iterations):\n",
    "        cost_history = np.zeros(n_iterations)\n",
    "        for i in range(n_iterations):\n",
    "            theta -= learning_rate * self.grad(X, y, theta)\n",
    "            cost_history[i] = self.cost_function(X, y, theta)\n",
    "        return theta, cost_history\n",
    "\n",
    "    def plot_learning_curve(self):\n",
    "        plt.plot(range(len(self.cost_history)), self.cost_history)\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Learning Curve')\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        def coef_determination(y, pred):\n",
    "            u = ((y - pred) ** 2).sum()\n",
    "            v = ((y - y.mean()) ** 2).sum()\n",
    "            return 1 - u / v\n",
    "\n",
    "        self.cd_train = coef_determination(self.y_train, self.model(self.X_train_poly_norm, self.theta_final))\n",
    "        self.cd_test = coef_determination(self.y_test, self.model(self.X_test_poly_norm, self.theta_final))\n",
    "\n",
    "        print(\"Coefficient de détermination de training set =\", self.cd_train)\n",
    "        print(\"Coefficient de détermination de test set =\", self.cd_test)\n",
    "\n",
    "    def l1_regularization(self, lambda_, learning_rate=0.05, n_iterations=10000):\n",
    "        def cost_function_L1(X, y, theta, lambda_):\n",
    "            m = len(y)\n",
    "            error = self.model(X, theta) - y\n",
    "            regularization_term = lambda_ * np.sum(np.abs(theta))\n",
    "            cost = (1 / (2 * m)) * np.sum(error ** 2) + regularization_term\n",
    "            return cost\n",
    "\n",
    "        def grad_L1(X, y, theta, lambda_):\n",
    "            m = len(y)\n",
    "            error = self.model(X, theta) - y\n",
    "            regularization_term = lambda_ * np.sign(theta)\n",
    "            grad = (1 / m) * X.T.dot(error) + regularization_term\n",
    "            return grad\n",
    "\n",
    "        def gradient_descent_L1(X, y, theta, lambda_, learning_rate, n_iterations):\n",
    "            cost_history = np.zeros(n_iterations)\n",
    "            for i in range(n_iterations):\n",
    "                theta -= learning_rate * grad_L1(X, y, theta, lambda_)\n",
    "                cost_history[i] = cost_function_L1(X, y, theta, lambda_)\n",
    "            return theta, cost_history\n",
    "\n",
    "        self.theta_final_L1, self.cost_history_L1 = gradient_descent_L1(self.X_train_poly_norm, self.y_train, self.theta.copy(), lambda_, learning_rate, n_iterations)\n",
    "\n",
    "    def l2_regularization(self, lambda_, learning_rate=0.05, n_iterations=10000):\n",
    "        def cost_function_L2(X, y, theta, lambda_):\n",
    "            m = len(y)\n",
    "            error = self.model(X, theta) - y\n",
    "            regularization_term = lambda_ * np.sum(theta ** 2)\n",
    "            cost = (1 / (2 * m)) * np.sum(error ** 2) + regularization_term\n",
    "            return cost\n",
    "\n",
    "        def grad_L2(X, y, theta, lambda_):\n",
    "            m = len(y)\n",
    "            error = self.model(X, theta) - y\n",
    "            regularization_term = lambda_ * theta\n",
    "            grad = (1 / m) * X.T.dot(error) + regularization_term\n",
    "            return grad\n",
    "\n",
    "        def gradient_descent_L2(X, y, theta, lambda_, learning_rate, n_iterations):\n",
    "            cost_history = np.zeros(n_iterations)\n",
    "            for i in range(n_iterations):\n",
    "                theta -= learning_rate * grad_L2(X, y, theta, lambda_)\n",
    "                cost_history[i] = cost_function_L2(X, y, theta, lambda_)\n",
    "            return theta, cost_history\n",
    "\n",
    "        self.theta_final_L2, self.cost_history_L2 = gradient_descent_L2(self.X_train_poly_norm, self.y_train, self.theta.copy(), lambda_, learning_rate, n_iterations)\n",
    "\n",
    "    def compare_regularizations(self):\n",
    "        def coef_determination(y, pred):\n",
    "            u = ((y - pred) ** 2).sum()\n",
    "            v = ((y - y.mean()) ** 2).sum()\n",
    "            return 1 - u / v\n",
    "\n",
    "        cd_train_L1 = coef_determination(self.y_train, self.model(self.X_train_poly_norm, self.theta_final_L1))\n",
    "        cd_test_L1 = coef_determination(self.y_test, self.model(self.X_test_poly_norm, self.theta_final_L1))\n",
    "        cd_train_L2 = coef_determination(self.y_train, self.model(self.X_train_poly_norm, self.theta_final_L2))\n",
    "        cd_test_L2 = coef_determination(self.y_test, self.model(self.X_test_poly_norm, self.theta_final_L2))\n",
    "\n",
    "        print(\"L1 Regularization - Coefficient de détermination de training set =\", cd_train_L1)\n",
    "        print(\"L1 Regularization - Coefficient de détermination de test set =\", cd_test_L1)\n",
    "        print(\"L2 Regularization - Coefficient de détermination de training set =\", cd_train_L2)\n",
    "        print(\"L2 Regularization - Coefficient de détermination de test set =\", cd_test_L2)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Instantiate the PolynomialRegressionModel class\n",
    "    model = PolynomialRegressionModel(data_path=\"mouse_viral_study.csv\", target_col=\"Med_2_mL\", feature_col=\"Med_1_mL\")\n",
    "\n",
    "    # Step 1: Load and preprocess data\n",
    "    model.load_data()\n",
    "\n",
    "    # Step 2: Plot correlation matrix\n",
    "    model.plot_corr_matrix()\n",
    "\n",
    "    # Step 3: Plot data\n",
    "    model.plot_data()\n",
    "\n",
    "    # Step 4: Select best polynomial degree\n",
    "    model.select_best_polynomial_degree()\n",
    "\n",
    "    # Step 5: Prepare polynomial features\n",
    "    model.prepare_polynomial_features(degree=model.best_degree)\n",
    "\n",
    "    # Step 6: Train model\n",
    "    model.train_model()\n",
    "\n",
    "    # Step 7: Plot learning curve\n",
    "    model.plot_learning_curve()\n",
    "\n",
    "    # Step 8: Evaluate model\n",
    "    model.evaluate_model()\n",
    "\n",
    "    # Step 9: Apply L1 regularization\n",
    "    model.l1_regularization(lambda_=0.017)\n",
    "\n",
    "    # Step 10: Apply L2 regularization\n",
    "    model.l2_regularization(lambda_=0.017)\n",
    "\n",
    "    # Step 11: Compare regularizations\n",
    "    model.compare_regularizations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def initialization(X):\n",
    "    W = np.random.randn(X.shape[1], 1)\n",
    "    b = np.random.randn(1)\n",
    "    return W, b\n",
    "\n",
    "def model(X, W, b):\n",
    "    Z = X.dot(W) + b\n",
    "    A = sigmoid(Z)\n",
    "    return A\n",
    "\n",
    "def log_loss(y, A):\n",
    "    return 1/len(y) * np.sum(-y * np.log(A) - (1 - y) * np.log(1 - A))\n",
    "\n",
    "def gradients(X, A, y):\n",
    "    dW = 1/len(y) * np.dot(X.T, A - y)\n",
    "    db = 1/len(y) * np.sum(A - y)\n",
    "    return dW, db\n",
    "\n",
    "def optimization(X, W, b, A, y, learning_rate):\n",
    "    dW, db = gradients(X, A, y)\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    return W, b\n",
    "\n",
    "def predict(X, W, b):\n",
    "    A = model(X, W, b)\n",
    "    return A >= 0.5\n",
    "\n",
    "def logistic_regression(X, y, learning_rate=0.1, n_iter=10000):\n",
    "    W, b = initialization(X)\n",
    "    loss_history = []\n",
    "    for i in range(n_iter):\n",
    "        A = model(X, W, b)\n",
    "        loss_history.append(log_loss(y, A))\n",
    "        W, b = optimization(X, W, b, A, y, learning_rate)\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('n_iteration')\n",
    "    plt.ylabel('Log_loss')\n",
    "    plt.title('Evolution des erreurs')\n",
    "    return W, b\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset = pd.read_csv(\"../mouse_viral_study.csv\")\n",
    "    X = dataset[['Med_1_mL', 'Med_2_mL']].values\n",
    "    y = dataset[['Virus Present']].values\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train logistic regression model\n",
    "    W, b = logistic_regression(X_train, y_train, learning_rate=0.1, n_iter=10000)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred_test = predict(X_test, W, b)\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    print(\"Accuracy =\", accuracy)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    fig, ax = plt.subplots(figsize=(9, 6))\n",
    "    ax.scatter(X_train[:,0], X_train[:, 1], c=y_train, cmap='winter')\n",
    "    x1 = np.linspace(0, 10, 200)\n",
    "    x2 = (-W[0] * x1 - b) / W[1]\n",
    "    ax.plot(x1, x2, c='orange', lw=3)\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"size\": 30}, fmt='d', cmap=\"Blues\", ax=ax)\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def mapFeature(self, X1, X2, degree):\n",
    "        res = np.ones(X1.shape[0])\n",
    "        for i in range(1, degree + 1):\n",
    "            for j in range(0, i + 1):\n",
    "                res = np.column_stack((res, (X1 ** (i - j)) * (X2 ** j)))\n",
    "        return res\n",
    "    \n",
    "    def fit(self, X_train, y_train, degree=1):\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_normalized = self.scaler.fit_transform(X_train)\n",
    "        Poly_X_train = self.mapFeature(X_train_normalized[:, 0], X_train_normalized[:, 1], degree)\n",
    "        self.model = LogisticRegression(max_iter=10000)\n",
    "        self.model.fit(Poly_X_train, y_train)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_test_normalized = self.scaler.transform(X_test)\n",
    "        Poly_X_test = self.mapFeature(X_test_normalized[:, 0], X_test_normalized[:, 1], degree)\n",
    "        return self.model.predict(Poly_X_test)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(\"Accuracy:\", np.round(accuracy * 100), \"%\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset = pd.read_csv(\"./iris/iris.csv\")\n",
    "    X = dataset[['petal_length', 'petal_width']].values\n",
    "    y = dataset[['class']].values\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model = CustomLogisticRegression()\n",
    "    model.fit(X_train, y_train, degree=5)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.evaluate(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
